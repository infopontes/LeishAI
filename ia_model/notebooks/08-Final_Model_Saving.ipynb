{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b959aa-bdf4-4496-989d-aa298e1ad867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Final Training and Saving Pipeline ---\n",
      "Loading and preprocessing data...\n",
      "Data preprocessing complete.\n",
      "Data split. Training set has 43 features.\n",
      "\n",
      "Training the final EasyEnsembleClassifier (n_estimators=50)...\n",
      "Final model training completed in 2.11 seconds.\n",
      "\n",
      "--- Saving the Best Performing Model and Columns ---\n",
      "Model successfully saved to: ../models/leish_model_v1.joblib\n",
      "Training columns (total: 43) saved to: ../models/training_columns_v1.joblib\n",
      "\n",
      "--- Artifacts saved. IA Model phase is complete. ---\n"
     ]
    }
   ],
   "source": [
    "# --- 12. Final Model Training and Saving ---\n",
    "# This cell loads, preprocesses, and trains the final champion model\n",
    "# (EasyEnsembleClassifier n=50) and saves the artifacts for the API.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.ensemble import EasyEnsembleClassifier # Our champion model\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "print(\"--- Running Final Training and Saving Pipeline ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. Load and Preprocess Data (Standard Block) ---\n",
    "# ==============================================================================\n",
    "print(\"Loading and preprocessing data...\")\n",
    "file_path = '../data/raw/leish_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df_processed = df.copy()\n",
    "\n",
    "for col in df_processed.select_dtypes(include=['object']).columns:\n",
    "    df_processed[col] = df_processed[col].fillna('Unknown')\n",
    "target_map = {'positivo': 1, 'negativo': 0, 'Unknown': 0}\n",
    "df_processed['diagnosis'] = df_processed['diagnosis'].map(target_map).astype(int)\n",
    "X_categorical = df_processed.drop('diagnosis', axis=1)\n",
    "y_numeric = df_processed['diagnosis']\n",
    "X_numeric = pd.get_dummies(X_categorical, drop_first=True, dtype=int)\n",
    "print(\"Data preprocessing complete.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. Split Data (to get final training columns) ---\n",
    "# ==============================================================================\n",
    "# We split the data here mainly to get the final list of feature names (X_train.columns)\n",
    "# that our API will need to use for preprocessing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_numeric, y_numeric, test_size=0.2, random_state=42, stratify=y_numeric\n",
    ")\n",
    "print(f\"Data split. Training set has {X_train.shape[1]} features.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. Train the Champion Model ---\n",
    "# ==============================================================================\n",
    "print(\"\\nTraining the final EasyEnsembleClassifier (n_estimators=50)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# We define our final model with the champion parameters\n",
    "final_model = EasyEnsembleClassifier(\n",
    "    n_estimators=50, \n",
    "    random_state=42,\n",
    "    n_jobs=1, # Keep it stable\n",
    "    verbose=0  # No need for verbose logging in the final run\n",
    ")\n",
    "# We fit the model on the training data\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Final model training completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. Save Model and Column Artifacts ---\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Saving the Best Performing Model and Columns ---\")\n",
    "\n",
    "# Define paths to save the artifacts\n",
    "model_dir = Path('../models')\n",
    "model_file = model_dir / 'leish_model_v1.joblib'\n",
    "columns_file = model_dir / 'training_columns_v1.joblib'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(final_model, model_file)\n",
    "print(f\"Model successfully saved to: {model_file}\")\n",
    "\n",
    "# Save the list of training columns\n",
    "# This is CRITICAL for the API to preprocess new data correctly\n",
    "training_columns = X_train.columns.tolist()\n",
    "joblib.dump(training_columns, columns_file)\n",
    "print(f\"Training columns (total: {len(training_columns)}) saved to: {columns_file}\")\n",
    "\n",
    "print(\"\\n--- Artifacts saved. IA Model phase is complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb656ac-b3e1-464a-83c2-5e9cbf675c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
