{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b289d8b2-abd9-45ba-a55c-57fea6ebb32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preprocessing Data ---\n",
      "--- Data Preprocessing Complete ---\n",
      "Shape of numerical features (X): (456, 43)\n",
      "\n",
      "--- Data Split ---\n",
      "Training set shape: (364, 43)\n",
      "Test set shape: (92, 43)\n",
      "Original positive cases in training set: 109 (29.9%)\n",
      "\n",
      "--- Applying SMOTE within Pipelines ---\n",
      "\n",
      "--- Training Logistic Regression with SMOTE ---\n",
      "\n",
      "--- Logistic Regression (SMOTE) Performance ---\n",
      "Confusion Matrix:\n",
      "[[44 21]\n",
      " [14 13]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72        65\n",
      "           1       0.38      0.48      0.43        27\n",
      "\n",
      "    accuracy                           0.62        92\n",
      "   macro avg       0.57      0.58      0.57        92\n",
      "weighted avg       0.65      0.62      0.63        92\n",
      "\n",
      "\n",
      "--- Training XGBoost with SMOTE ---\n",
      "\n",
      "--- XGBoost (SMOTE) Performance ---\n",
      "Confusion Matrix:\n",
      "[[45 20]\n",
      " [16 11]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.69      0.71        65\n",
      "           1       0.35      0.41      0.38        27\n",
      "\n",
      "    accuracy                           0.61        92\n",
      "   macro avg       0.55      0.55      0.55        92\n",
      "weighted avg       0.63      0.61      0.62        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced Techniques for Imbalanced Learning Notebook ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE # Import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Import Pipeline from imblearn\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. Load and Preprocess the Data ---\n",
    "# ==============================================================================\n",
    "print(\"--- Loading and Preprocessing Data ---\")\n",
    "\n",
    "# Load the raw dataset\n",
    "file_path = '../data/raw/leish_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Handle Missing Values\n",
    "for col in df_processed.select_dtypes(include=['object']).columns:\n",
    "    df_processed[col] = df_processed[col].fillna('Unknown')\n",
    "\n",
    "# Encode the Target Variable\n",
    "target_map = {'positivo': 1, 'negativo': 0, 'Unknown': 0}\n",
    "df_processed['diagnosis'] = df_processed['diagnosis'].map(target_map).astype(int)\n",
    "\n",
    "# Separate features from the target BEFORE encoding features\n",
    "X_categorical = df_processed.drop('diagnosis', axis=1)\n",
    "y_numeric = df_processed['diagnosis'] # Use a different name for the numeric target\n",
    "\n",
    "# Apply One-Hot Encoding to categorical features\n",
    "X_numeric = pd.get_dummies(X_categorical, drop_first=True, dtype=int)\n",
    "\n",
    "print(\"--- Data Preprocessing Complete ---\")\n",
    "print(f\"Shape of numerical features (X): {X_numeric.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. Split Data AFTER Preprocessing ---\n",
    "# ==============================================================================\n",
    "# Now we split the fully numeric X_numeric and y_numeric\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_numeric, y_numeric, test_size=0.2, random_state=42, stratify=y_numeric\n",
    ")\n",
    "print(\"\\n--- Data Split ---\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "# This print statement will now work correctly\n",
    "print(f\"Original positive cases in training set: {y_train.sum()} ({y_train.mean()*100:.1f}%)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. Apply SMOTE within Pipelines ---\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Applying SMOTE within Pipelines ---\")\n",
    "\n",
    "# --- Model 1: Logistic Regression with SMOTE ---\n",
    "pipeline_logreg = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"\\n--- Training Logistic Regression with SMOTE ---\")\n",
    "pipeline_logreg.fit(X_train, y_train)\n",
    "y_pred_logreg_smote = pipeline_logreg.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Logistic Regression (SMOTE) Performance ---\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_logreg_smote))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg_smote))\n",
    "\n",
    "# --- Model 2: XGBoost with SMOTE ---\n",
    "pipeline_xgb = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\n--- Training XGBoost with SMOTE ---\")\n",
    "pipeline_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_smote = pipeline_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n--- XGBoost (SMOTE) Performance ---\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb_smote))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc11fd-2f0b-461e-a7ab-885e7956542e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
